\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Pa}{\mathrm{Pa}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Adj}{\mathrm{Adj}}
\newcommand{\Oo}{\mathcal{O}}

\title{Exact Distributed Structure-Learning for Bayesian Networks}
%Distributed Learning for Large Scale Bayesian Networks by D-separation Approach
% The role of causal hidden variables
\author{}

\begin{document}
\maketitle

\begin{abstract}
Learning the structure of a Bayesian network can be achieved through either constraint-based approaches that test conditional independencies between variables or score-based approaches that find the network maximizing a likelihood-based function. However, these approaches are only practical for a limited number of variables due to their high computational costs. Existing distributed learning approaches approximate the true structure. We present an exact distributed structure-learning algorithm that consists of three phases. First, the algorithm partitions the variables into independent sets using d-separation. Then, in parallel, it finds a locally optimum structure for each partition using either constraint or score-based methods. Third, the local structures are concatenated using information from the first step. The resulting network matches that of a centralized learning algorithm, provided that an exact learning algorithm is used on each partition. The minimum number of variables in each partition equals the maximum number of parents in the network, allowing for a significant reduction in computation time, particularly for sparse networks.
\end{abstract}

\section{Introduction}
The constraint-based approach is a major class of structure learning algorithms such as PC and FCI that is based on independence (dependence) detection []. 
Constraint-based algorithms with sufficiency, Markov, and faithfulness assumptions are based on finding (conditional) independencies by
with a test to gauge the likelihood of such independencies.
The result of this approach is class of independence-equivalent (\(I\)-equivalent) graphs represented as a partial DAG (PDAG). 
Under the Markov and faithfulness assumptions, constraint-based methods have been shown to asymptotically output the correct PDAG []. 

The PC algorithm is one of the primary constraint-based approaches.
It first finds an undirected graph by starting from the
complete graph and iteratively removing edges
based on a series of conditional independence (CI) tests to obtain; it then
orients some of these edges using information from the conditional independencies.

The computational complexity of the number of conditional independence tests is $\Oo(2^N)$ in the worst case, where $N$ is the number of variables. 
That said, in many practical problems (namely, those with sparse), the PC algorithm 
can use fewer conditional independence tests, as removing edges reduces
the number of tests needed later in the algorithm.

This exponential complexity limits the use of the PC algorithm for non-sparse
graphs with more than a handful of variables. We present an algorithm that
allows for a distributed approach to this problem, increasing the maximum
viable network size for such algorithms.

\section{Background}
For describing conditional independencies between two subsets of the nodes in a DAG, d-separation defines the CI between every two subsets of nodes with respect to a third subset of nodes as follows:
\begin{definition} [d-separation] \label{definition_d-separation}
    Consider the DAG $\G$ with node set $\V$.
    A trail $\T$ between two nodes $X$ and $Y$ in $\V$ is \emph{active} relative to a set of nodes $\Z$ if 
    \begin{enumerate}
    	\item every non-collider on $\T$ is not a member of $\Z$
    	\item every collider on $t$ is an ancestor of some member of $\Z$
    \end{enumerate}
    The node subsets $\X$ and $\Y$ are \emph{d-separated} given the subset $\Z$, if there is no active
    trail between any node $X \in \X$ and any node $Y \in \Y$ given $\Z$.
\end{definition}
If $\X$ and $\Y$ are \emph{d-separated} given $\Z$, denoted $dsep_{\G}(\X,\Y \mid \Z )$, the paths between $\X$ and $\Y$ are blocked by $\Z$ and we say $\X$ is independent to $\Y$ given $\Z$. $\I (\G)$ is the set of all independencies corresponding to d-separation.
Let $\I(P)$ denote the set of all conditional independencies implied by the distribution $P$.
The Markovian and faithfulness assumptions give us a correspondence between $\I(\G)$ and $\I(P)$.

The Makovian assumption is as follows:
\begin{assumption}[Markovian] \label{assumption_Markovness}
    $\I(\G)\subseteq \I(P)$.
\end{assumption}
However, the Markov condition (MC) is insufficient to learn Bayesian networks from observational data because, for example, a fully connected DAG has $\I(\G) = \emptyset$ and hence satisfies MC for any observational distribution.
Thus some other assumption connecting \(\G\) and \(P\) is needed.
A common assumption is \emph{faithfulness}, the converse of the MC. 
\begin{assumption}[faithfulness] \label{assumption_faithfulness}
    $\mathcal{I}(P) \subseteq \mathcal{I}(\mathcal{G})$.
\end{assumption}
The distribution $P$ is said to be \emph{faithful} to the DAG $\G$ if it satisfies the above assumption. Also, if $\I(\G) = \I(P)$, as implied by the
two assumptions, then $\G$ is a P-map (perfect-map) for $P$. 

\section{Partitioning of variables}
Consider the simple Bayesian network in Fig.\! \ref{f-simple} (a). By using d-separation, this network can be segmented into several sub-networks. For example, if \(X_3\) is observed, this network is partitioned into the three segments shown in Fig.\! \ref{f-simple} (b). In this case, we have
\begin{equation*}
    X_1 \perp X_4 \mid X_3~~,~~X_1 \perp X_5 \mid X_3~~,~~X_2 \perp X_4 \mid X_3~~,~~X_2 \perp X_5 \mid X_3~~,~~X_4 \perp X_5 \mid X_3
\end{equation*}
whereas 
\begin{equation*}
    X_1 \not\perp X_4~~, ~~X_1\not \perp X_5~~, ~~X_2 \not\perp X_4~~, ~~X_2\not \perp X_5~~, ~~X_4\not \perp X_5
\end{equation*}
and $X_1\not \perp X_2 \mid X_3$. So, $X_3$ plays a separator role between some variables. Then, three subsets of variables $\{X_1, X_2, X_3\}$, $\{X_3, X_4\}$ and $\{X_3,X_5\}$ obtain by using d-separation. 

By using the matrix representation of conditional independencies, the dependency matrix of \(X_3\) for the ordered vector of variables $V^o_{X_3} = [X_1, X_2, X_4, X_5]$ can be defined as
\begin{equation*}
D_{X_3} = \left[{\begin{array}{*{20}{c}} 
1 & 1 & 0 & 0\\
1 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{array}}\right]
\end{equation*}
$D_{X_3}$ has three diagonal blocks corresponding to three segments of the graph partitioning. If the variables are ordered as $\bar V^o_{X_3} = [X_1, X_4, X_2, X_5]$ then $\bar D_{X_3}$ is obtained as
\begin{equation*}
\bar D_{X_3} = \left[{\begin{array}{*{20}{c}} 
1 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{array}}\right]
\end{equation*}
By using the permutation matrix
\begin{equation*}
P = \left[{\begin{array}{*{20}{c}}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 0 & 1\\
\end{array}}\right]
\end{equation*}
we have 
\begin{equation}
    P\bar D_{X_3} P^{-1} = D_{X_3}~~~~~,~~~~~P\bar V^o_{X_3} = V^o_{X_3}
\end{equation}
Therefore, by using the dependency matrix $D_{X_3}$, three subsets $\{X_1,X_2,X_3\}$, $\{X_4,X_3\}$ and $\{X_5,X_3\}$ are obtain. As a result, a structure learning problem of five variables is changed to three structure learning problems with two or three variables. This approach leads to reducing the number of variables and run-time of the structure learning problem by using parallel computing. In other words, a large-scale structure learning problem can be converted to some small structure learning problems.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.30]{Dist-simple-exm.png}
    \caption{Caption}
    \label{f-simple}
\end{figure}


In the general form, consider random variable set $\X = \{X_1, X_2,\cdots, X_N \}$. The computational complexity to learn by using constraint-based  approaches is $\mathcal{O}(2^N)$.  If the number of variables $N$ is large, the structure learning will be an NP-hard problem and it cannot be solved. 
Considering the explained strategy, $\X$ can be partitioned to some subsets with fewer elements by using conditional independencies and d-separation. 

For partitioning $\X$ by $\W \subset \X$, we can use conditional independence tests. For each two variables $X_i,X_j \in \X$ if $X_i\perp X_j \mid \W$ and $X_i\not\perp X_j$ then $X_i$ and $X_j$ are not in the same partition, but if $X_i\not \perp X_j \mid \W$ then $X_i$ and $X_j$ are in the same partition with respect to $\W$.
Therefore, the union of all $X_i$ that belong to the same partition and the observed variables set $\W$ make a partition. 
But if $X_i\perp X_j \mid \W$ and $X_i\perp X_j$ then $\W$ cannot use for partitioning.

The other way for partitioning is the usage of the dependency matrix. In this method, an order for variables is considered and a vector $V^o = [X_1, X_2, \cdots, X_N] \in R^n$ is constructed. Then for $W \subset X$ dependency matrix $D_W$ is calculated such that $D_W(i,i) = 1$ for all $i$ and $D_W(i,j) = 1$ if $X_i\not \perp X_j \mid W$ and $D_W(i,j) = 0$ if $X_i\perp X_j \mid W$ for $i\not = j$. So $D_W$ is a symmetric $(N-\mid W \mid) \times (N-\mid W \mid)$ matrix. Also, $V^o_W$ by removing the elements of $W$ from $V^o$ is obtained. By using the permutation matrix, we have
$$D_W = P\bar D_W P^{-1} $$
where $P$ is a transformation matrix that is calculated by multiplication of permutation matrices and $\bar D_W$ is a block diagonal matrix of $D_W$. Therefore, $\bar V^o_W = P V^o_W$ can be used to partition variables with respect to $W$ considering $\bar D_W$.
\par\noindent\rule{\textwidth}{0.5pt}
\vspace{-0.4cm}
Algorithm: Finding the best partitioning
\vspace{0.2cm}
\par\noindent\rule{\textwidth}{0.5pt}
\begin{itemize}
\item Set $\X_P = \{\X\}$
    \item Set the value of $M$ as the maximum acceptable number of variables in partitions
    \item Set the value of $d$ as the maximum number of separator variables 
    \item for $j = 1, \cdots , d$
    \begin{itemize}
        \item for $i = 1,\cdots, N$
            \begin{enumerate}         
                \item partition all $U\in \X_P$ given $X_U \in U$ with cardinality of $i$ by using conditional independencies or dependency matrix
                \item update $\X_P$ as the set of all partitions
            \end{enumerate}
        \item If ($max \mid U \mid < M$ for all $U\in \X_P$) or $(j == d)$:\\
        stop algorithm and return $\X_P$ as variables sets for partitioning\\
        else:\\
        j = j + 1
        
    \end{itemize}
\end{itemize}
\vspace{-0.4cm}
\par\noindent\rule{\textwidth}{0.5pt}

After partitioning, the structure learning method is applied for each partition of variables separately and the structure network of each partition is found. For constructing the complete structure network, local structures must be concatenated with each other. Each partitioning has a common variable set which is $\W$ (observed variables) that is corresponding to some nodes in local networks. So, if local networks are put together by common nodes, the complete structure network for all variables is made.

For example, consider the naive Bayes network in Fig.\ref{f-naive}. We know, $X_i\perp X_j \mid Y$ and $X_i\not\perp X_j$ for each $i\not = j$. So, the variables set $\{Y,X_1,X_2,\cdots, X_n\}$ is partitioned to $X_P = \{ \{X_1,Y\}, \{X_2,Y\}, \cdots, \{X_n,Y\}$ by observation of $Y$. Consequently, we solve $n$ structure learning problems with two variables instead of one problem with $n+1$ variables.

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.30]{naive bayes.png}
    \caption{Caption}
    \label{f-naive}
\end{figure}

But for the v-stare structure, Fig.\ref{f-vstare}, we know from probability distribution that $X_i\perp X_j$ and  $X_i\not\perp X_j\mid Y$ for each $i\not = j$. therefore, this structure cannot be partitioned by the proposed algorithm. Therefore, the set of all variables $\{Y,X_1,X_2,\cdots, X_n\}$ will be one partition. As a result, it seems that the minimum number of variables in each partition is limited by the maximum number of parents in the network.
%Also, for a v-structure in a network the effect variable is independent of the other variables given the cause variables.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.30]{v-structure.png}
    \caption{Caption}
    \label{f-vstare}
\end{figure}

To investigate the powerful of d-separation usage in the PC approach, let's check the Alarm data set [] problem with the true graph in Fig. \ref{Alarm} as a practical example.
In this problem, the number of variables is $N=37$. If we use the PC algorithm, it will need $O(2^N)$ computational burden that for the Alarm problem we need about $2^{37}=1.37\times 10^{11}$ conditional dependence tests.
By using d-separation by conditional independence tests on one conditional variable that needs $O(N^3)$ number of conditional independence tests, which is $37^3=50653$ conditional independence test computations for partitioning. By using the variables of the set $\{VENTTUBE, PULMEMBOLUS, TPR, STROKEVOLUME, HR \}$ all variables divide into 6 parts so that the largest part has 21 variables, according to Fig.\ref{Alarm1}. Therefore, we need about $2^{21}=2.1\times 10^6$ conditional independence test computations. Consequently, by using only one iteration of d-separation the computational burden is reduced from $2^{37}=1.37\times 10^{11}$ to $2^{21}=2.1\times 10^6$.

Also, if we do another iteration of the proposed algorithm it will need to check all conditional independencies on two variables for the largest part in the previous step that has 21 variables. Therefore, the computational burden for separation by two variables will be $O(N^4)$, which is on the order of $21^4=194481$.
As a result, this separation divides the variable set of the previous largest part into three parts which the largest part has 14 variables (see Fig. \ref{Alarm2}). Thus the computational burden to use the PC algorithm in this step will be $O(2^14)$ that presents it needs $16348$ conditional independence test computations to find true structure.
Consequently, by using two iterations of d-separation the order of the computational burden is reduced from $2^{37}=1.37\times 10^{11}$ to $21^{4}=194481$.

\begin{figure}
    \centering
    \includegraphics[scale=1]{Alarm.png}
    \caption{Alarm}
    \label{Alarm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=.9]{Alarmcutcolor.png}
    % \caption{Caption}
    \label{Alarm1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=.9]{Alarmcut2color.png}
    % \caption{Caption}
    \label{Alarm2}
\end{figure}



\end{document}


% Learning the structure of a Bayesian network is done either by constraint-based approaches, where conditional independencies between variables are tested, and score-based approaches, where the network that maximizes a likelihood-based function is found. 
% However, both approaches can be applied to a limited number of variables in practice due to the high computational costs. 
% Existing distributed learning approaches approximate the ``true'' structure. 
% We provide an exact distributed structure-learning algorithm consisting of three phases. 
% First, it uses d-separation to partition the variables into independent sets.
% Then the algorithm in parallel finds a locally optimum structure for each partition by either constraint or score-based methods. 
% Third, the local structures are concatenated by using the information from the first part. 
% The resulting network matches that of a centralized learning algorithm, provided that an exact learning algorithm is used for each partition.
% The minimum number of variables in each partition equals the maximum number of parents in the network, allowing for significant reduction in the computation time, particularly for sparse networks. 








